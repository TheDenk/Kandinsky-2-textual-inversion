{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9394ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2a725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import PIL\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from omegaconf.dictconfig import DictConfig\n",
    "from huggingface_hub import hf_hub_url, cached_download\n",
    "\n",
    "from kandinsky2 import CONFIG_2_1, Kandinsky2_1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632fc5cc",
   "metadata": {},
   "source": [
    "### Training parameters \n",
    "#### Necessarily fill initializer_token, placeholder_token, data_root and out_folder. You can change other parameters if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7935b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "task_type = 'text2img'\n",
    "cache_root = '/tmp/kandinsky2'\n",
    "\n",
    "# Fill here -------------------------------------------------------\n",
    "initializer_token = 'train' # Global object name (train, bear, etc)\n",
    "placeholder_token = 'sapsan' # Unique object name (sapsan, *, etc)\n",
    "data_root = './finetune/input/sapsan/instance_images' # your folder with images\n",
    "out_folder = './finetune/output/sapsan' # your folder for saved embeddings\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "img_size = 512\n",
    "epochs = 3000\n",
    "log_image_frequency = 250 # -1 disable image logging\n",
    "log_embed_frequency = 250\n",
    "\n",
    "lr = 1e-4\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "weight_decay = 1e-2\n",
    "epsilon = 1e-08\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37209edc",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9695e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_models_if_not_exist(\n",
    "    task_type=\"text2img\",\n",
    "    cache_dir=\"/tmp/kandinsky2\",\n",
    "    use_auth_token=None,\n",
    "):\n",
    "    cache_dir = os.path.join(cache_dir, \"2_1\")\n",
    "    if task_type == \"text2img\":\n",
    "        model_name = \"decoder_fp16.ckpt\"\n",
    "        config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=model_name)\n",
    "    elif task_type == \"inpainting\":\n",
    "        model_name = \"inpainting_fp16.ckpt\"\n",
    "        config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=model_name)\n",
    "    cached_download(\n",
    "        config_file_url,\n",
    "        cache_dir=cache_dir,\n",
    "        force_filename=model_name,\n",
    "        use_auth_token=use_auth_token,\n",
    "    )\n",
    "    prior_name = \"prior_fp16.ckpt\"\n",
    "    config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=prior_name)\n",
    "    cached_download(\n",
    "        config_file_url,\n",
    "        cache_dir=cache_dir,\n",
    "        force_filename=prior_name,\n",
    "        use_auth_token=use_auth_token,\n",
    "    )\n",
    "    cache_dir_text_en = os.path.join(cache_dir, \"text_encoder\")\n",
    "    for name in [\n",
    "        \"config.json\",\n",
    "        \"pytorch_model.bin\",\n",
    "        \"sentencepiece.bpe.model\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "    ]:\n",
    "        config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=f\"text_encoder/{name}\")\n",
    "        cached_download(\n",
    "            config_file_url,\n",
    "            cache_dir=cache_dir_text_en,\n",
    "            force_filename=name,\n",
    "            use_auth_token=use_auth_token,\n",
    "        )\n",
    "    config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=\"movq_final.ckpt\")\n",
    "    cached_download(\n",
    "        config_file_url,\n",
    "        cache_dir=cache_dir,\n",
    "        force_filename=\"movq_final.ckpt\",\n",
    "        use_auth_token=use_auth_token,\n",
    "    )\n",
    "    config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=\"ViT-L-14_stats.th\")\n",
    "    cached_download(\n",
    "        config_file_url,\n",
    "        cache_dir=cache_dir,\n",
    "        force_filename=\"ViT-L-14_stats.th\",\n",
    "        use_auth_token=use_auth_token,\n",
    "    )\n",
    "    \n",
    "def add_noise(original_samples, noise, timesteps):\n",
    "    num_diffusion_timesteps = 1000\n",
    "    scale = 1000 / num_diffusion_timesteps\n",
    "    beta_start = scale * 0.00085\n",
    "    beta_end = scale * 0.012\n",
    "        \n",
    "    betas = torch.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=original_samples.dtype)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        \n",
    "    alphas_cumprod = alphas_cumprod.to(device=original_samples.device, dtype=original_samples.dtype)\n",
    "    timesteps = timesteps.to(original_samples.device)\n",
    "\n",
    "    sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5\n",
    "    sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n",
    "    while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n",
    "        sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "    sqrt_one_minus_alpha_prod = (1 - alphas_cumprod[timesteps]) ** 0.5\n",
    "    sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n",
    "    while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n",
    "        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "    noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n",
    "    return noisy_samples\n",
    "\n",
    "def generate_clip_emb(model,\n",
    "        prompt,\n",
    "        batch_size=1,\n",
    "        prior_cf_scale=1,\n",
    "        prior_steps=\"5\",\n",
    "        negative_prior_prompt=\"\",\n",
    "    ):\n",
    "    prompts_batch = [prompt for _ in range(batch_size)]\n",
    "    prior_cf_scales_batch = [prior_cf_scale] * len(prompts_batch)\n",
    "    prior_cf_scales_batch = torch.tensor(prior_cf_scales_batch, device=model.device)\n",
    "    max_txt_length = model.prior.model.text_ctx\n",
    "    tok, mask = model.tokenizer2.padded_tokens_and_mask(\n",
    "        prompts_batch, max_txt_length\n",
    "    )\n",
    "    cf_token, cf_mask = model.tokenizer2.padded_tokens_and_mask(\n",
    "        [negative_prior_prompt], max_txt_length\n",
    "    )\n",
    "    if not (cf_token.shape == tok.shape):\n",
    "        cf_token = cf_token.expand(tok.shape[0], -1)\n",
    "        cf_mask = cf_mask.expand(tok.shape[0], -1)\n",
    "    tok = torch.cat([tok, cf_token], dim=0)\n",
    "    mask = torch.cat([mask, cf_mask], dim=0)\n",
    "    tok, mask = tok.to(device=model.device), mask.to(device=model.device)\n",
    "    x = model.clip_model.token_embedding(tok).type(model.clip_model.dtype)\n",
    "    x = x + model.clip_model.positional_embedding.type(model.clip_model.dtype)\n",
    "    x = x.permute(1, 0, 2)  # NLD -> LND|\n",
    "    x = model.clip_model.transformer(x)\n",
    "    x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "    x = model.clip_model.ln_final(x).type(model.clip_model.dtype)\n",
    "    txt_feat_seq = x\n",
    "    txt_feat = (x[torch.arange(x.shape[0]), tok.argmax(dim=-1)] @ model.clip_model.text_projection)\n",
    "    txt_feat, txt_feat_seq = txt_feat.float().to(model.device), txt_feat_seq.float().to(model.device)\n",
    "    \n",
    "    img_feat = model.prior(\n",
    "        txt_feat,\n",
    "        txt_feat_seq,\n",
    "        mask,\n",
    "        prior_cf_scales_batch,\n",
    "        timestep_respacing=prior_steps,\n",
    "    )\n",
    "    return img_feat.to(model.model_dtype)\n",
    "\n",
    "\n",
    "def save_embeds(model, save_path, placeholder_token, t1_place_token_id, t2_place_token_id):\n",
    "    t1_embeds = model.text_encoder.model.transformer.get_input_embeddings().weight[t1_place_token_id]\n",
    "    t2_embeds = model.clip_model.token_embedding.weight[t2_place_token_id]\n",
    "    learned_embeds_dict = {\n",
    "        't1': {\n",
    "            placeholder_token: t1_embeds.cpu().detach(), \n",
    "        },\n",
    "        't2':{\n",
    "            placeholder_token: t2_embeds.cpu().detach(),\n",
    "        },\n",
    "    }\n",
    "    torch.save(learned_embeds_dict, save_path)\n",
    "    \n",
    "    \n",
    "def save_images(model, save_path, placeholder_token, img_size=512):\n",
    "    gen_images = model.generate_text2img(\n",
    "            f\"a photo of a {placeholder_token}\",\n",
    "            num_steps=50, \n",
    "            batch_size=4, \n",
    "            guidance_scale=7.5,\n",
    "            h=img_size, \n",
    "            w=img_size,\n",
    "            sampler=\"p_sampler\", \n",
    "            prior_cf_scale=4,\n",
    "            prior_steps=\"5\",\n",
    "        )\n",
    "\n",
    "    gen_images = np.hstack([np.array(img) for img in gen_images])\n",
    "    Image.fromarray(gen_images).save(save_path)\n",
    "\n",
    "def check_tokens_is_valid(model, placeholder_token, initializer_token):\n",
    "    print(\"Check tokens...\")\n",
    "    if placeholder_token in model.tokenizer2.encoder: \n",
    "        raise ValueError(f\"Word {placeholder_token} exists in tokenizer2. Please select another word.\")\n",
    "\n",
    "    if initializer_token not in model.tokenizer2.encoder:  \n",
    "        raise ValueError(f\"Word {initializer_token} doesn't exist in tokenizer2. Please select another word.\")\n",
    "\n",
    "    if len(model.tokenizer1.encode(placeholder_token)) == 3: \n",
    "        raise ValueError(f\"Word {placeholder_token} exists in tokenizer1. Please select another word.\")\n",
    "\n",
    "    if len(model.tokenizer1.encode(initializer_token)) != 3: \n",
    "        raise ValueError(f\"Word {initializer_token} doesn't exists in tokenizer1. Please select another word.\")\n",
    "    print(\"Selected tokens are correct\")\n",
    "\n",
    "def show_image(image, figsize=(5, 5), cmap=None, title='', xlabel=None, ylabel=None, axis=False):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(image, cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.axis(axis)\n",
    "    plt.show();\n",
    "\n",
    "def show_images(images, n_rows=1, title='', figsize=(5, 5), cmap=None, xlabel=None, ylabel=None, axis=False):\n",
    "    n_cols = len(images) // n_rows\n",
    "    if n_rows == n_cols == 1:\n",
    "        show_image(images[0], title=title, figsize=figsize, cmap=cmap, xlabel=xlabel, ylabel=ylabel, axis=axis)\n",
    "    else:\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "        fig.tight_layout(pad=0.0)\n",
    "        axes = axes.flatten()\n",
    "        for ax, img in zip(axes, images):\n",
    "            ax.imshow(img, cmap=cmap)\n",
    "            ax.set_title(title)\n",
    "            ax.set_xlabel(xlabel)\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.axis(axis)\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5017e24e",
   "metadata": {},
   "source": [
    "### Dataset class functions\n",
    "Text templates and dataset class with small changes from <a href=\"https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion\">diffusers</a> repo:  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7346c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_templates_small = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a rendering of a {}\",\n",
    "    \"a cropped photo of the {}\",\n",
    "    \"the photo of a {}\",\n",
    "    \"a photo of a clean {}\",\n",
    "    \"a photo of a dirty {}\",\n",
    "    \"a dark photo of the {}\",\n",
    "    \"a photo of my {}\",\n",
    "    \"a photo of the cool {}\",\n",
    "    \"a close-up photo of a {}\",\n",
    "    \"a bright photo of the {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a photo of the {}\",\n",
    "    \"a good photo of the {}\",\n",
    "    \"a photo of one {}\",\n",
    "    \"a close-up photo of the {}\",\n",
    "    \"a rendition of the {}\",\n",
    "    \"a photo of the clean {}\",\n",
    "    \"a rendition of a {}\",\n",
    "    \"a photo of a nice {}\",\n",
    "    \"a good photo of a {}\",\n",
    "    \"a photo of the nice {}\",\n",
    "    \"a photo of the small {}\",\n",
    "    \"a photo of the weird {}\",\n",
    "    \"a photo of the large {}\",\n",
    "    \"a photo of a cool {}\",\n",
    "    \"a photo of a small {}\",\n",
    "]\n",
    "\n",
    "imagenet_style_templates_small = [\n",
    "    \"a painting in the style of {}\",\n",
    "    \"a rendering in the style of {}\",\n",
    "    \"a cropped painting in the style of {}\",\n",
    "    \"the painting in the style of {}\",\n",
    "    \"a clean painting in the style of {}\",\n",
    "    \"a dirty painting in the style of {}\",\n",
    "    \"a dark painting in the style of {}\",\n",
    "    \"a picture in the style of {}\",\n",
    "    \"a cool painting in the style of {}\",\n",
    "    \"a close-up painting in the style of {}\",\n",
    "    \"a bright painting in the style of {}\",\n",
    "    \"a cropped painting in the style of {}\",\n",
    "    \"a good painting in the style of {}\",\n",
    "    \"a close-up painting in the style of {}\",\n",
    "    \"a rendition in the style of {}\",\n",
    "    \"a nice painting in the style of {}\",\n",
    "    \"a small painting in the style of {}\",\n",
    "    \"a weird painting in the style of {}\",\n",
    "    \"a large painting in the style of {}\",\n",
    "]\n",
    "\n",
    "class TextualInversionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root,\n",
    "        placeholder_token,\n",
    "        img_size=512,\n",
    "        learnable_property=\"object\", # [object, style]\n",
    "        flip_p=0.5,\n",
    "        center_crop=False,\n",
    "    ):\n",
    "        self.data_root = data_root\n",
    "        self.img_size = img_size\n",
    "        self.placeholder_token = placeholder_token\n",
    "        self.center_crop = center_crop\n",
    "        self.flip_p = flip_p\n",
    "\n",
    "        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n",
    "        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n",
    "        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        example = {}\n",
    "        image = Image.open(self.image_paths[i])\n",
    "\n",
    "        if not image.mode == \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "\n",
    "        placeholder_string = self.placeholder_token\n",
    "        text = random.choice(self.templates).format(placeholder_string)\n",
    "        example[\"text\"] = text\n",
    "        \n",
    "        img = np.array(image).astype(np.uint8)\n",
    "\n",
    "        if self.center_crop:\n",
    "            h, w = img.shape[:2]\n",
    "            min_side = min(h, w)\n",
    "            img = img[(h - min_side) // 2 : (h + min_side) // 2, (w - min_side) // 2 : (w + min_side) // 2]\n",
    "\n",
    "        image = Image.fromarray(img)\n",
    "        image = image.resize((self.img_size, self.img_size), resample=PIL.Image.Resampling.BICUBIC)\n",
    "\n",
    "        image = self.flip_transform(image)\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "\n",
    "        example[\"image\"] = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cced5f",
   "metadata": {},
   "source": [
    "### Define config and create Kandinsky model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c2783e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/huggingface_hub/file_download.py:629: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n"
     ]
    }
   ],
   "source": [
    "config = DictConfig(deepcopy(CONFIG_2_1))\n",
    "\n",
    "cache_dir = os.path.join(cache_root, \"2_1\")\n",
    "\n",
    "config[\"model_config\"][\"up\"] = False\n",
    "config[\"model_config\"][\"use_fp16\"] = False\n",
    "config[\"model_config\"][\"inpainting\"] = False\n",
    "config[\"model_config\"][\"cache_text_emb\"] = False\n",
    "config[\"model_config\"][\"use_flash_attention\"] = False\n",
    "\n",
    "config[\"tokenizer_name\"] = os.path.join(cache_dir, \"text_encoder\")\n",
    "config[\"text_enc_params\"][\"model_path\"] = os.path.join(cache_dir, \"text_encoder\")\n",
    "config[\"prior\"][\"clip_mean_std_path\"] = os.path.join(cache_dir, \"ViT-L-14_stats.th\")\n",
    "config[\"image_enc_params\"][\"ckpt_path\"] = os.path.join(cache_dir, \"movq_final.ckpt\")\n",
    "\n",
    "model_path = os.path.join(cache_dir, \"decoder_fp16.ckpt\")\n",
    "prior_path = os.path.join(cache_dir, \"prior_fp16.ckpt\")\n",
    "\n",
    "download_models_if_not_exist(task_type=task_type, cache_dir=cache_root)\n",
    "\n",
    "model = Kandinsky2_1(config, model_path, prior_path, device, task_type=task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9f2679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check tokens...\n",
      "Selected tokens are correct\n"
     ]
    }
   ],
   "source": [
    "check_tokens_is_valid(model, placeholder_token, initializer_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46ffff6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num added tokens: 1\n",
      "Initializer ID: 25550 | Placeholder token ID: 250002\n"
     ]
    }
   ],
   "source": [
    "# Convert the initializer_token and placeholder_token to tokenizer1 ids\n",
    "num_added_tokens = model.tokenizer1.add_tokens(placeholder_token)\n",
    "print(f'Num added tokens: {num_added_tokens}')\n",
    "\n",
    "t1_init_token_id = model.tokenizer1.encode(initializer_token, add_special_tokens=False)[0]\n",
    "t1_place_token_id = model.tokenizer1.convert_tokens_to_ids(placeholder_token)\n",
    "\n",
    "model.text_encoder.model.transformer.resize_token_embeddings(len(model.tokenizer1))\n",
    "\n",
    "print(f'Initializer ID: {t1_init_token_id} | Placeholder token ID: {t1_place_token_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edb7ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise new placeholder weights with the embeddings of the initializer token\n",
    "token_embeds = model.text_encoder.model.transformer.get_input_embeddings().weight.data\n",
    "token_embeds[t1_place_token_id] = token_embeds[t1_init_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df6b8693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode placeholder token: 49408 | Decode placeholder word: lastochka\n"
     ]
    }
   ],
   "source": [
    "# Convert the initializer_token, placeholder_token to ids for tokenizer2\n",
    "# and add placeholder_token to tokenizer2\n",
    "t2p_index_to_add = len(model.tokenizer2.encoder)\n",
    "model.tokenizer2.encoder[placeholder_token] = t2p_index_to_add\n",
    "model.tokenizer2.decoder[t2p_index_to_add] = placeholder_token\n",
    "model.tokenizer2.cache[placeholder_token] = placeholder_token\n",
    "\n",
    "t2_place_token_id = model.tokenizer2.encode(placeholder_token)[0]\n",
    "t2_place_token_str = model.tokenizer2.decode([t2_place_token_id])\n",
    "\n",
    "print(f'Encode placeholder token: {t2_place_token_id} | Decode placeholder word: {t2_place_token_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fa5087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode initializer token: 3231 | Decode initializer word: train \n",
      "T2 old vocab size: 49408 | T2 Embed size: 768\n",
      "T2 new vocab size: 49409\n"
     ]
    }
   ],
   "source": [
    "# 1.Convert the initializer_token and placeholder_token to tokenizer2 ids\n",
    "# 2.Create new embeddings \n",
    "# 3.Copy old weights to the new embeddings and initialize new token \n",
    "    \n",
    "t2_init_token_id = model.tokenizer2.encode(initializer_token)[0]\n",
    "t2_init_token_str = model.tokenizer2.decode([t2_init_token_id])\n",
    "print(f'Encode initializer token: {t2_init_token_id} | Decode initializer word: {t2_init_token_str}')\n",
    "\n",
    "old_vocab_size, t2_embed_size = model.clip_model.token_embedding.weight.shape\n",
    "print(f'T2 old vocab size: {old_vocab_size} | T2 Embed size: {t2_embed_size}')\n",
    "\n",
    "new_embed = nn.Embedding(old_vocab_size + 1, t2_embed_size).to(device)\n",
    "new_embed.weight.data[:old_vocab_size, :] = model.clip_model.token_embedding.weight.data.clone()\n",
    "new_embed.weight.data[t2_place_token_id, :] = new_embed.weight.data[t2_init_token_id, :]\n",
    "\n",
    "model.clip_model.token_embedding = deepcopy(new_embed)\n",
    "\n",
    "print(f'T2 new vocab size: {model.clip_model.token_embedding.weight.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7a014e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freeze all except embeddings\n",
    "model.image_encoder.requires_grad_(False)\n",
    "model.model.requires_grad_(False)\n",
    "model.prior.requires_grad_(False)\n",
    "\n",
    "model.clip_model.token_embedding.requires_grad_(True)\n",
    "model.clip_model.transformer.requires_grad_(False);\n",
    "\n",
    "model.text_encoder.model.transformer.get_input_embeddings().requires_grad_(True)\n",
    "model.text_encoder.model.transformer.embeddings.position_embeddings.requires_grad_(False)\n",
    "model.text_encoder.model.transformer.embeddings.token_type_embeddings.requires_grad_(False)\n",
    "model.text_encoder.model.transformer.encoder.requires_grad_(False)\n",
    "model.text_encoder.model.transformer.pooler.requires_grad_(False);\n",
    "model.text_encoder.model.LinearTransformation.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1b6810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure optimizer, dataset and dataloader\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(model.text_encoder.model.transformer.get_input_embeddings().parameters()) +\n",
    "    list(model.clip_model.token_embedding.parameters()),\n",
    "    lr=lr,\n",
    "    betas=(beta1, beta2),\n",
    "    weight_decay=weight_decay,\n",
    "    eps=epsilon,\n",
    ")\n",
    "\n",
    "dataset = TextualInversionDataset(\n",
    "    data_root=data_root,\n",
    "    placeholder_token=placeholder_token,\n",
    "    img_size=img_size,\n",
    "    center_crop=False,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49eaa2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save original embeddings from both models\n",
    "orig_t1_params = model.text_encoder.model.transformer.get_input_embeddings().weight.data.clone()\n",
    "orig_t2_params = model.clip_model.token_embedding.weight.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "513320ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_dtype = model.model.dtype\n",
    "model.clip_model.to(weight_dtype)\n",
    "\n",
    "progress_bar_epochs = tqdm(range(1, epochs + 1))\n",
    "\n",
    "for epoch in progress_bar_epochs:\n",
    "    model.text_encoder.train()\n",
    "    model.clip_model.train()\n",
    "    for batch in train_dataloader:\n",
    "        model_kwargs = {}\n",
    "        # Convert images to latent representation and add noise\n",
    "        latents = model.image_encoder.encode(batch[\"image\"].to(device=device, dtype=weight_dtype)).detach()\n",
    "        latents = latents * model.scale\n",
    "\n",
    "        noise = torch.randn_like(latents)\n",
    "\n",
    "        timesteps = torch.randint(0, 1000, (batch_size,), device=latents.device)\n",
    "        timesteps = timesteps.long()\n",
    "\n",
    "        noisy_latents = add_noise(latents, noise, timesteps).to(weight_dtype)\n",
    "        \n",
    "        # Get hidden parameters for both models\n",
    "        # First Model Parameters\n",
    "        image_emb = generate_clip_emb(\n",
    "            model,\n",
    "            batch[\"text\"][0],\n",
    "            batch_size=batch_size,\n",
    "            prior_cf_scale=4,\n",
    "            prior_steps=\"5\",\n",
    "            negative_prior_prompt=\"\",\n",
    "        )\n",
    "        \n",
    "        model_kwargs[\"image_emb\"] = image_emb.to(weight_dtype)\n",
    "        \n",
    "        # Second Model Parameters\n",
    "        tokens = model.tokenizer1(\n",
    "            batch[\"text\"][0],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        model_kwargs[\"full_emb\"], model_kwargs[\"pooled_emb\"] = model.text_encoder(\n",
    "            tokens=tokens['input_ids'].long().to(device=device), \n",
    "            mask=tokens['attention_mask'].to(device=device),\n",
    "        )\n",
    "\n",
    "        model_kwargs[\"full_emb\"] = model_kwargs[\"full_emb\"].to(weight_dtype) \n",
    "        model_kwargs[\"pooled_emb\"] = model_kwargs[\"pooled_emb\"].to(weight_dtype) \n",
    "        \n",
    "        # Predict noise obviously\n",
    "        model_pred = model.model(noisy_latents, timesteps, **model_kwargs)[:, :4]\n",
    "\n",
    "        loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # We don't need update all embeddings weights. Only new embeddings.\n",
    "        with torch.no_grad():\n",
    "            index_no_updates_t1 = torch.arange(len(model.tokenizer1)) != t1_place_token_id\n",
    "            model.text_encoder.model.transformer.get_input_embeddings().weight[\n",
    "                index_no_updates_t1\n",
    "            ] = orig_t1_params[index_no_updates_t1]\n",
    "            \n",
    "            index_no_updates_t2 = torch.arange(model.clip_model.token_embedding.weight.shape[0]) != t2_place_token_id\n",
    "            model.clip_model.token_embedding.weight[\n",
    "                index_no_updates_t2\n",
    "            ] = orig_t2_params[index_no_updates_t2]\n",
    "            \n",
    "        progress_bar_epochs.set_postfix(**{\"loss\": loss.cpu().detach().item()})\n",
    "    \n",
    "    if epoch % log_embed_frequency == 0:\n",
    "        embed_save_path = os.path.join(out_folder, f\"{epoch}_epoch_embeds.bin\")\n",
    "        save_embeds(model, embed_save_path, placeholder_token, t1_place_token_id, t2_place_token_id)\n",
    "        \n",
    "    if log_image_frequency > 0 and (epoch % log_image_frequency == 0):\n",
    "        images_root = os.path.join(out_folder, \"images\")\n",
    "        os.makedirs(images_root, exist_ok=True)\n",
    "        image_save_path = os.path.join(images_root, f\"{epoch}_epoch_images.jpg\")\n",
    "        save_images(model, image_save_path, placeholder_token)\n",
    "        \n",
    "        \n",
    "embed_save_path = os.path.join(out_folder, \"learned_embeds.bin\")\n",
    "save_embeds(model, embed_save_path, placeholder_token, t1_place_token_id, t2_place_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014effde",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = model.generate_text2img(\n",
    "            f'photo of {placeholder_token}',\n",
    "            num_steps=50, \n",
    "            batch_size=2, \n",
    "            guidance_scale=7.5,\n",
    "            h=img_size, \n",
    "            w=img_size,\n",
    "            sampler=\"ddim_sampler\", \n",
    "            prior_cf_scale=4,\n",
    "            prior_steps=\"8\",\n",
    "        )\n",
    "show_images(images, n_rows=1, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b3c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = model.generate_text2img(\n",
    "            f'Professional high-quality photo of {placeholder_token}. photorealistic, 4k, HQ',\n",
    "            num_steps=50, \n",
    "            batch_size=2, \n",
    "            guidance_scale=7.5,\n",
    "            h=img_size, \n",
    "            w=img_size,\n",
    "            sampler=\"ddim_sampler\", \n",
    "            prior_cf_scale=4,\n",
    "            prior_steps=\"8\",\n",
    "        )\n",
    "show_images(images, n_rows=1, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a3543b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e2ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c6fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
