{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9394ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1deb7",
   "metadata": {},
   "source": [
    "#### Current code adapted from <a href=\"https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth.py\">diffusers</a>  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2a725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import random\n",
    "import hashlib\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import PIL\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from omegaconf.dictconfig import DictConfig\n",
    "from huggingface_hub import hf_hub_url, cached_download\n",
    "\n",
    "from kandinsky2 import CONFIG_2_1, Kandinsky2_1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632fc5cc",
   "metadata": {},
   "source": [
    "### Training parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7935b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "task_type = 'text2img'\n",
    "cache_root = '/tmp/kandinsky2'\n",
    "\n",
    "# Fill here -------------------------------------------------------\n",
    "class_prompt = 'train' # Global object name (train, bear, etc)\n",
    "instance_prompt = 'sapsan' # Unique object name (sapsan, *, etc)\n",
    "class_data_dir = './finetune/input/sapsan'  # folder with your images\n",
    "instance_data_dir = './finetune/input/sapsan/instance_images' # folder with generated images for prior loss\n",
    "out_folder = './finetune/output/sapsan' # your folder for saved model and images\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# Pretrained weights -----------------------------------------------\n",
    "# model_path = os.path.join(out_folder, \"decoder_fp16.ckpt\") # None if not exists\n",
    "model_path = None  # Original Kandinsky-2.1 unet if None\n",
    "\n",
    "# Generated Images -----------------------------------------------\n",
    "# csv_path =  os.path.join(class_data_dir, \"class_csv.csv\") # None if not exists\n",
    "csv_path = None  # Create new images if None\n",
    "\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "img_size = 512\n",
    "epochs = 4\n",
    "log_image_frequency = -1 # -1 disable image logging\n",
    "log_model_frequency = 50\n",
    "\n",
    "prior_loss_weight = 1.0\n",
    "num_class_images = 256\n",
    "center_crop = False\n",
    "\n",
    "lr = 5e-6\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "weight_decay = 1e-2\n",
    "epsilon = 1e-08\n",
    "\n",
    "num_workers = 0\n",
    "train_batch_size = 1\n",
    "sample_batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37209edc",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9695e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, figsize=(5, 5), cmap=None, title='', xlabel=None, ylabel=None, axis=False):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(image, cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.axis(axis)\n",
    "    plt.show();\n",
    "\n",
    "def show_images(images, n_rows=1, title='', figsize=(5, 5), cmap=None, xlabel=None, ylabel=None, axis=False):\n",
    "    n_cols = len(images) // n_rows\n",
    "    if n_rows == n_cols == 1:\n",
    "        show_image(images[0], title=title, figsize=figsize, cmap=cmap, xlabel=xlabel, ylabel=ylabel, axis=axis)\n",
    "    else:\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "        fig.tight_layout(pad=0.0)\n",
    "        axes = axes.flatten()\n",
    "        for ax, img in zip(axes, images):\n",
    "            ax.imshow(img, cmap=cmap)\n",
    "            ax.set_title(title)\n",
    "            ax.set_xlabel(xlabel)\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.axis(axis)\n",
    "        plt.show();\n",
    "        \n",
    "def download_models_if_not_exist(\n",
    "    task_type=\"text2img\",\n",
    "    cache_dir=\"/tmp/kandinsky2\",\n",
    "    use_auth_token=None,\n",
    "):\n",
    "    cache_dir = os.path.join(cache_dir, \"2_1\")\n",
    "    if task_type == \"text2img\":\n",
    "        model_name = \"decoder_fp16.ckpt\"\n",
    "        config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=model_name)\n",
    "    elif task_type == \"inpainting\":\n",
    "        model_name = \"inpainting_fp16.ckpt\"\n",
    "        config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=model_name)\n",
    "    cached_download(\n",
    "        config_file_url,\n",
    "        cache_dir=cache_dir,\n",
    "        force_filename=model_name,\n",
    "        use_auth_token=use_auth_token,\n",
    "    )\n",
    "    prior_name = \"prior_fp16.ckpt\"\n",
    "    config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=prior_name)\n",
    "    cached_download(\n",
    "        config_file_url,\n",
    "        cache_dir=cache_dir,\n",
    "        force_filename=prior_name,\n",
    "        use_auth_token=use_auth_token,\n",
    "    )\n",
    "    cache_dir_text_en = os.path.join(cache_dir, \"text_encoder\")\n",
    "    for name in [\n",
    "        \"config.json\",\n",
    "        \"pytorch_model.bin\",\n",
    "        \"sentencepiece.bpe.model\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "    ]:\n",
    "        config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=f\"text_encoder/{name}\")\n",
    "        cached_download(\n",
    "            config_file_url,\n",
    "            cache_dir=cache_dir_text_en,\n",
    "            force_filename=name,\n",
    "            use_auth_token=use_auth_token,\n",
    "        )\n",
    "    config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=\"movq_final.ckpt\")\n",
    "    cached_download(\n",
    "        config_file_url,\n",
    "        cache_dir=cache_dir,\n",
    "        force_filename=\"movq_final.ckpt\",\n",
    "        use_auth_token=use_auth_token,\n",
    "    )\n",
    "    config_file_url = hf_hub_url(repo_id=\"sberbank-ai/Kandinsky_2.1\", filename=\"ViT-L-14_stats.th\")\n",
    "    cached_download(\n",
    "        config_file_url,\n",
    "        cache_dir=cache_dir,\n",
    "        force_filename=\"ViT-L-14_stats.th\",\n",
    "        use_auth_token=use_auth_token,\n",
    "    )\n",
    "    \n",
    "def add_noise(original_samples, noise, timesteps):\n",
    "    num_diffusion_timesteps = 1000\n",
    "    scale = 1000 / num_diffusion_timesteps\n",
    "    beta_start = scale * 0.00085\n",
    "    beta_end = scale * 0.012\n",
    "        \n",
    "    betas = torch.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=original_samples.dtype)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        \n",
    "    alphas_cumprod = alphas_cumprod.to(device=original_samples.device, dtype=original_samples.dtype)\n",
    "    timesteps = timesteps.to(original_samples.device)\n",
    "\n",
    "    sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5\n",
    "    sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n",
    "    while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n",
    "        sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "    sqrt_one_minus_alpha_prod = (1 - alphas_cumprod[timesteps]) ** 0.5\n",
    "    sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n",
    "    while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n",
    "        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "    noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n",
    "    return noisy_samples\n",
    "    \n",
    "def save_images(model, save_path, instance_prompt, class_prompt, img_size=512):\n",
    "    simple_images = model.generate_text2img(\n",
    "            f\"a photo of a {instance_prompt} {class_prompt}\",\n",
    "            num_steps=50, \n",
    "            batch_size=2, \n",
    "            guidance_scale=7.5,\n",
    "            h=img_size, \n",
    "            w=img_size,\n",
    "            sampler=\"p_sampler\", \n",
    "            prior_cf_scale=4,\n",
    "            prior_steps=\"5\",\n",
    "    )\n",
    "    cool_images = model.generate_text2img(\n",
    "            f\"Professional high-quality photo of a {instance_prompt} {class_prompt}. photorealistic, 4k, HQ\",\n",
    "            num_steps=50, \n",
    "            batch_size=2, \n",
    "            guidance_scale=7.5,\n",
    "            h=img_size, \n",
    "            w=img_size,\n",
    "            sampler=\"p_sampler\", \n",
    "            prior_cf_scale=4,\n",
    "            prior_steps=\"5\",\n",
    "    )\n",
    "    images = [*simple_images, *cool_images]\n",
    "    instance_images = np.hstack([np.array(img) for img in images])\n",
    "    \n",
    "    simple_images = model.generate_text2img(\n",
    "            f\"a photo of a {class_prompt}\",\n",
    "            num_steps=50, \n",
    "            batch_size=2, \n",
    "            guidance_scale=7.5,\n",
    "            h=img_size, \n",
    "            w=img_size,\n",
    "            sampler=\"p_sampler\", \n",
    "            prior_cf_scale=4,\n",
    "            prior_steps=\"5\",\n",
    "    )\n",
    "    cool_images = model.generate_text2img(\n",
    "            f\"Professional high-quality photo of a {class_prompt}. photorealistic, 4k, HQ\",\n",
    "            num_steps=50, \n",
    "            batch_size=2, \n",
    "            guidance_scale=7.5,\n",
    "            h=img_size, \n",
    "            w=img_size,\n",
    "            sampler=\"p_sampler\", \n",
    "            prior_cf_scale=4,\n",
    "            prior_steps=\"5\",\n",
    "    )\n",
    "    images = [*simple_images, *cool_images]\n",
    "    class_images = np.hstack([np.array(img) for img in images])\n",
    "    gen_images = np.vstack([instance_images, class_images])\n",
    "    Image.fromarray(gen_images).save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7346c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts from original dreambooth repo + from diffusers + custom words\n",
    "# https://github.com/google/dreambooth/blob/main/dataset/prompts_and_classes.txt\n",
    "# https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth.py\n",
    "\n",
    "instance_templates = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a rendering of a {}\",\n",
    "    \"a cropped photo of the {}\",\n",
    "    \"the photo of a {}\",\n",
    "    \"a photo of a clean {}\",\n",
    "    \"a photo of a dirty {}\",\n",
    "    \"a dark photo of the {}\",\n",
    "    \"a photo of my {}\",\n",
    "    \"a photo of the cool {}\",\n",
    "    \"a close-up photo of a {}\",\n",
    "    \"a bright photo of the {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a photo of the {}\",\n",
    "    \"a good photo of the {}\",\n",
    "    \"a photo of one {}\",\n",
    "    \"a close-up photo of the {}\",\n",
    "    \"a rendition of the {}\",\n",
    "    \"a photo of the clean {}\",\n",
    "    \"a rendition of a {}\",\n",
    "    \"a photo of a nice {}\",\n",
    "    \"a good photo of a {}\",\n",
    "    \"a photo of the nice {}\",\n",
    "    \"a photo of the small {}\",\n",
    "    \"a photo of the weird {}\",\n",
    "    \"a photo of the large {}\",\n",
    "    \"a photo of a cool {}\",\n",
    "    \"a photo of a small {}\",\n",
    "]\n",
    "\n",
    "class_templates = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a rendering of a {}\",\n",
    "    \"a cropped photo of the {}\",\n",
    "    \"the photo of a {}\",\n",
    "    \"a photo of a clean {}\",\n",
    "    \"a photo of a dirty {}\",\n",
    "    \"a dark photo of the {}\",\n",
    "    \"a photo of my {}\",\n",
    "    \"a photo of the cool {}\",\n",
    "    \"a close-up photo of a {}\",\n",
    "    \"a bright photo of the {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a photo of the {}\",\n",
    "    \"a good photo of the {}\",\n",
    "    \"a photo of one {}\",\n",
    "    \"a close-up photo of the {}\",\n",
    "    \"a rendition of the {}\",\n",
    "    \"a photo of the clean {}\",\n",
    "    \"a rendition of a {}\",\n",
    "    \"a photo of a nice {}\",\n",
    "    \"a good photo of a {}\",\n",
    "    \"a photo of the nice {}\",\n",
    "    \"a photo of the small {}\",\n",
    "    \"a photo of the weird {}\",\n",
    "    \"a photo of the large {}\",\n",
    "    \"a photo of a cool {}\",\n",
    "    \"a photo of a small {}\",\n",
    "    'a {} in the jungle',\n",
    "    'a {} in the snow',\n",
    "    'a {} on the beach',\n",
    "    'a {} on a cobblestone street',\n",
    "    'a {} on top of pink fabric',\n",
    "    'a {} on top of a wooden floor',\n",
    "    'a {} with a city in the background',\n",
    "    'a {} with a mountain in the background',\n",
    "    'a {} with a blue house in the background',\n",
    "    'a {} on top of a purple rug in a forest',\n",
    "    'a {} with a wheat field in the background',\n",
    "    'a {} with a tree and autumn leaves in the background',\n",
    "    'a {} with the Eiffel Tower in the background',\n",
    "    'a {} floating on top of water',\n",
    "    'a {} floating in an ocean of milk',\n",
    "    'a {} on top of green grass with sunflowers around it',\n",
    "    'a {} on top of a mirror',\n",
    "    'a {} on top of the sidewalk in a crowded street',\n",
    "    'a {} on top of a dirt road',\n",
    "    'a {} on top of a white rug',\n",
    "    'a red {}',\n",
    "    'a purple {}',\n",
    "    'a shiny {}',\n",
    "    'a wet {}',\n",
    "    'a cube shaped {}',\n",
    "]\n",
    "           \n",
    "extend_words = [\n",
    "    'photorealistic', 'epic', 'high quality',\n",
    "    'cinematic', 'extremely high detail', \n",
    "    'cinematic lighting', 'trending on artstation', \n",
    "    'cgsociety', 'realistic rendering of Unreal Engine 5', \n",
    "    '8k', '4k', 'HQ', 'wallpaper',\n",
    "]\n",
    "\n",
    "def get_prompt_extention(extend_words):\n",
    "    total_samples = len(extend_words)\n",
    "    n_samples = random.randint(0, total_samples)\n",
    "    additional_samples = random.sample(extend_words, n_samples)\n",
    "    random.shuffle(additional_samples)\n",
    "    p_extention = ', '.join(additional_samples) if len(additional_samples) else ''\n",
    "    return p_extention\n",
    "\n",
    "class DreamBoothDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        instance_data_root,\n",
    "        instance_prompt,\n",
    "        class_prompt,\n",
    "        class_csv, # info about image_path and prompt\n",
    "        size=512,\n",
    "        center_crop=False,\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.center_crop = center_crop\n",
    "\n",
    "        self.instance_data_root = Path(instance_data_root)\n",
    "        if not self.instance_data_root.exists():\n",
    "            raise ValueError(f\"Instance {self.instance_data_root} images root doesn't exists.\")\n",
    "\n",
    "        self.instance_images_path = list(Path(instance_data_root).iterdir())\n",
    "        self.num_instance_images = len(self.instance_images_path)\n",
    "        self.instance_prompt = instance_prompt\n",
    "        self.class_prompt = class_prompt\n",
    "        self.class_csv = class_csv\n",
    "        self._length = class_csv.shape[0]\n",
    "\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n",
    "        if not instance_image.mode == \"RGB\":\n",
    "            instance_image = instance_image.convert(\"RGB\")\n",
    "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
    "        example[\"instance_prompt\"] = random.choice(instance_templates).format(f'{self.instance_prompt} {self.class_prompt}')  #f'a photo of {self.instance_prompt} {self.class_prompt}' \n",
    "\n",
    "        class_image = Image.open(class_csv.iloc[index % self._length]['image_path'])\n",
    "        class_prompt = class_csv.iloc[index % self._length]['prompt']\n",
    "\n",
    "        if not class_image.mode == \"RGB\":\n",
    "            class_image = class_image.convert(\"RGB\")\n",
    "\n",
    "        example[\"class_images\"] = self.image_transforms(class_image)\n",
    "        example[\"class_prompt\"] = class_prompt\n",
    "\n",
    "        return example\n",
    "    \n",
    "    \n",
    "def collate_fn(examples, with_prior_preservation=True):\n",
    "    input_prompt = [example[\"instance_prompt\"] for example in examples]\n",
    "    pixel_values = [example[\"instance_images\"] for example in examples]\n",
    "\n",
    "    if with_prior_preservation:\n",
    "        input_prompt += [example[\"class_prompt\"] for example in examples]\n",
    "        pixel_values += [example[\"class_images\"] for example in examples]\n",
    "\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "    batch = {\n",
    "        \"prompt\": input_prompt,\n",
    "        \"image\": pixel_values,\n",
    "    }\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cced5f",
   "metadata": {},
   "source": [
    "### Define config and create Kandinsky model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c2783e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/huggingface_hub/file_download.py:629: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = DictConfig(deepcopy(CONFIG_2_1))\n",
    "\n",
    "cache_dir = os.path.join(cache_root, \"2_1\")\n",
    "\n",
    "config[\"model_config\"][\"up\"] = False\n",
    "config[\"model_config\"][\"use_fp16\"] = False\n",
    "config[\"model_config\"][\"inpainting\"] = False\n",
    "config[\"model_config\"][\"cache_text_emb\"] = False\n",
    "config[\"model_config\"][\"use_flash_attention\"] = False\n",
    "\n",
    "config[\"tokenizer_name\"] = os.path.join(cache_dir, \"text_encoder\")\n",
    "config[\"text_enc_params\"][\"model_path\"] = os.path.join(cache_dir, \"text_encoder\")\n",
    "config[\"prior\"][\"clip_mean_std_path\"] = os.path.join(cache_dir, \"ViT-L-14_stats.th\")\n",
    "config[\"image_enc_params\"][\"ckpt_path\"] = os.path.join(cache_dir, \"movq_final.ckpt\")\n",
    "\n",
    "model_path = os.path.join(cache_dir, \"decoder_fp16.ckpt\") if model_path is None else model_path\n",
    "prior_path = os.path.join(cache_dir, \"prior_fp16.ckpt\")\n",
    "\n",
    "download_models_if_not_exist(task_type=task_type, cache_dir=cache_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb7622",
   "metadata": {},
   "source": [
    "### Generate class images for prior loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfabfadf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_csv = pd.read_csv(csv_path) if csv_path is not None else None\n",
    "\n",
    "if class_csv is None:\n",
    "    class_csv = pd.DataFrame(columns=['image_path', 'prompt'])\n",
    "cur_class_images = class_csv.shape[0]\n",
    "\n",
    "if cur_class_images < num_class_images:\n",
    "    model = Kandinsky2_1(config, model_path, prior_path, device, task_type=task_type)\n",
    "    \n",
    "    num_new_images = num_class_images - cur_class_images\n",
    "    print(f\"Number of class images to sample: {num_new_images}.\")\n",
    "\n",
    "    for index_example in tqdm(range(num_new_images // sample_batch_size), desc=\"Generating class images\"):\n",
    "        prompt = random.choice(class_templates).format(class_prompt) + '. ' + get_prompt_extention(extend_words)\n",
    "        images = model.generate_text2img(\n",
    "            prompt,\n",
    "            num_steps=50, \n",
    "            batch_size=sample_batch_size, \n",
    "            guidance_scale=7.5,\n",
    "            h=img_size, \n",
    "            w=img_size,\n",
    "            sampler=\"p_sampler\", \n",
    "            prior_cf_scale=4,\n",
    "            prior_steps=\"8\",\n",
    "        )\n",
    "        \n",
    "        for i, image in enumerate(images):\n",
    "            hash_image = hashlib.sha1(image.tobytes()).hexdigest()\n",
    "            image_filename = os.path.join(class_data_dir, 'class_images', f\"{index_example + cur_class_images}-{hash_image}.jpg\")\n",
    "            image.save(image_filename)\n",
    "            \n",
    "            class_csv = class_csv.append({\n",
    "                'image_path': image_filename,\n",
    "                'prompt': prompt\n",
    "            }, ignore_index=True)\n",
    "    \n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    cur_class_images += num_new_images\n",
    "    \n",
    "    class_csv.to_csv(f'{class_data_dir}/class_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "235fdc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Kandinsky2_1(config, model_path, prior_path, device, task_type=task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd6cd613",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freeze all except unet\n",
    "model.model.requires_grad_(True)\n",
    "\n",
    "model.image_encoder.requires_grad_(False);\n",
    "model.prior.requires_grad_(False);\n",
    "model.clip_model.requires_grad_(False);\n",
    "model.text_encoder.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1b6810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure optimizer, dataset and dataloader\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.model.parameters(),\n",
    "    lr=lr,\n",
    "    betas=(beta1, beta2),\n",
    "    weight_decay=weight_decay,\n",
    "    eps=epsilon,\n",
    ")\n",
    "\n",
    "train_dataset = DreamBoothDataset(\n",
    "    instance_data_root=instance_data_dir,\n",
    "    instance_prompt=instance_prompt,\n",
    "    class_prompt=class_prompt,\n",
    "    class_csv=class_csv,\n",
    "    size=img_size,\n",
    "    center_crop=center_crop,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0db95eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clip_emb(\n",
    "        model,\n",
    "        prompts_batch,\n",
    "        negative_prompts_batch,\n",
    "        prior_cf_scale=1,\n",
    "        prior_steps=\"5\",\n",
    "    ):\n",
    "    prior_cf_scales_batch = [prior_cf_scale] * len(prompts_batch)\n",
    "    prior_cf_scales_batch = torch.tensor(prior_cf_scales_batch, device=model.device)\n",
    "    max_txt_length = model.prior.model.text_ctx\n",
    "    tok, mask = model.tokenizer2.padded_tokens_and_mask(\n",
    "        prompts_batch, max_txt_length\n",
    "    )\n",
    "    cf_token, cf_mask = model.tokenizer2.padded_tokens_and_mask(\n",
    "        negative_prompts_batch, max_txt_length\n",
    "    )\n",
    "    if not (cf_token.shape == tok.shape):\n",
    "        cf_token = cf_token.expand(tok.shape[0], -1)\n",
    "        cf_mask = cf_mask.expand(tok.shape[0], -1)\n",
    "    tok = torch.cat([tok, cf_token], dim=0)\n",
    "    mask = torch.cat([mask, cf_mask], dim=0)\n",
    "    tok, mask = tok.to(device=model.device), mask.to(device=model.device)\n",
    "    x = model.clip_model.token_embedding(tok).type(model.clip_model.dtype)\n",
    "    x = x + model.clip_model.positional_embedding.type(model.clip_model.dtype)\n",
    "    x = x.permute(1, 0, 2)  # NLD -> LND|\n",
    "    x = model.clip_model.transformer(x)\n",
    "    x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "    x = model.clip_model.ln_final(x).type(model.clip_model.dtype)\n",
    "    txt_feat_seq = x\n",
    "    txt_feat = (x[torch.arange(x.shape[0]), tok.argmax(dim=-1)] @ model.clip_model.text_projection)\n",
    "    txt_feat, txt_feat_seq = txt_feat.float().to(model.device), txt_feat_seq.float().to(model.device)\n",
    "    \n",
    "    img_feat = model.prior(\n",
    "        txt_feat,\n",
    "        txt_feat_seq,\n",
    "        mask,\n",
    "        prior_cf_scales_batch,\n",
    "        timestep_respacing=prior_steps,\n",
    "    )\n",
    "    return img_feat.to(model.model_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "513320ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_dtype = model.model.dtype\n",
    "model.clip_model.to(weight_dtype)\n",
    "\n",
    "progress_bar_epochs = tqdm(range(1, epochs + 1))\n",
    "\n",
    "for epoch in progress_bar_epochs:\n",
    "    model.model.train()\n",
    "    for batch in train_dataloader:\n",
    "        model_kwargs = {}\n",
    "        # Convert images to latent representation and add noise\n",
    "        latents = model.image_encoder.encode(batch[\"image\"].to(device=device, dtype=weight_dtype))\n",
    "        latents = latents * model.scale\n",
    "        timesteps = torch.randint(0, 1000, (train_batch_size,), device=latents.device)\n",
    "        timesteps = timesteps.long()\n",
    "        noise = torch.randn_like(latents)\n",
    "        noisy_latents = add_noise(latents, noise, timesteps).to(weight_dtype)\n",
    "        \n",
    "        image_emb = generate_clip_emb(\n",
    "            model,\n",
    "            batch[\"prompt\"],\n",
    "            [\"\" for _ in range(train_batch_size)],\n",
    "            prior_cf_scale=4,\n",
    "            prior_steps=\"8\",\n",
    "        )\n",
    "        \n",
    "        model_kwargs[\"image_emb\"] = image_emb.to(weight_dtype)\n",
    "        \n",
    "        # Second Model Parameters\n",
    "        tokens = model.tokenizer1(\n",
    "            batch[\"prompt\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        model_kwargs[\"full_emb\"], model_kwargs[\"pooled_emb\"] = model.text_encoder(\n",
    "            tokens=tokens['input_ids'].long().to(device=device), \n",
    "            mask=tokens['attention_mask'].to(device=device),\n",
    "        )\n",
    "\n",
    "        model_kwargs[\"full_emb\"] = model_kwargs[\"full_emb\"].to(weight_dtype) \n",
    "        model_kwargs[\"pooled_emb\"] = model_kwargs[\"pooled_emb\"].to(weight_dtype) \n",
    "        \n",
    "        # Predict noise obviously\n",
    "        model_pred = model.model(noisy_latents, timesteps, **model_kwargs)[:, :4]\n",
    "        \n",
    "        model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\n",
    "        target, target_prior = torch.chunk(noise, 2, dim=0)\n",
    "\n",
    "        # Compute instance loss\n",
    "        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "        # Compute prior loss\n",
    "        prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction=\"mean\")\n",
    "\n",
    "        # Add the prior loss to the instance loss.\n",
    "        loss = loss + prior_loss_weight * prior_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        progress_bar_epochs.set_postfix(**{\"loss\": loss.cpu().detach().item()})\n",
    "        \n",
    "    if log_image_frequency > 0 and (epoch % log_image_frequency == 0):\n",
    "        images_root = os.path.join(out_folder, \"images\")\n",
    "        os.makedirs(images_root, exist_ok=True)\n",
    "        image_save_path = os.path.join(images_root, f\"{epoch}_epoch_images.jpg\")\n",
    "        save_images(model, image_save_path, instance_prompt, class_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef9a4c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our new unet model\n",
    "out_model_path = os.path.join(out_folder, \"decoder_fp16.ckpt\")\n",
    "torch.save(model.model.state_dict(), out_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f568d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = model.generate_text2img(\n",
    "            f'photo of {instance_prompt} {class_prompt}',\n",
    "            num_steps=50, \n",
    "            batch_size=2, \n",
    "            guidance_scale=7.5,\n",
    "            h=img_size, \n",
    "            w=img_size,\n",
    "            sampler=\"p_sampler\", \n",
    "            prior_cf_scale=4,\n",
    "            prior_steps=\"8\",\n",
    "        )\n",
    "show_images(images, n_rows=1, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788e2ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = model.generate_text2img(\n",
    "            f'Professional high-quality photo of {instance_prompt} {class_prompt}. photorealistic, 4k, HQ',\n",
    "            num_steps=50, \n",
    "            batch_size=2, \n",
    "            guidance_scale=7.5,\n",
    "            h=img_size, \n",
    "            w=img_size,\n",
    "            sampler=\"ddim_sampler\", \n",
    "            prior_cf_scale=4,\n",
    "            prior_steps=\"8\",\n",
    "        )\n",
    "show_images(images, n_rows=1, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76d2c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = model.generate_text2img(\n",
    "            f'a photo of a helicopter {instance_prompt} {class_prompt}.',\n",
    "            num_steps=50, \n",
    "            batch_size=2, \n",
    "            guidance_scale=7.5,\n",
    "            h=img_size, \n",
    "            w=img_size,\n",
    "            sampler=\"ddim_sampler\", \n",
    "            prior_cf_scale=4,\n",
    "            prior_steps=\"8\",\n",
    "        )\n",
    "\n",
    "show_images(images, n_rows=1, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1890fb30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5453f92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9bb50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
